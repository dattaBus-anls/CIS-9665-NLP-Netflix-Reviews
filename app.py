import streamlit as st
import joblib

# ------------------------------------------------------------
# Load trained NLP model (TF-IDF + Logistic Regression pipeline)
# ------------------------------------------------------------
MODEL_PATH = "model.joblib"
model = joblib.load(MODEL_PATH)

st.set_page_config(
    page_title="Netflix Review Rating Predictor",
    page_icon="ğŸ¬",
    layout="centered"
)

# Header
st.markdown("""
### CIS 9665 â€“ Applied Natural Language Processing
**Instructor:** Chaoqun Deng  
**Student:** Apu Datta  

##### An NLP Analysis of Linguistic Patterns in Netflix User Reviews
""")

# -------------------------
# Tabs
# -------------------------
tab1, tab2 = st.tabs(["â­ Review Predictor", "ğŸ“Š Findings & Charts"])

# =========================
# TAB 1: Predictor + Intro
# =========================
with tab1:
    st.title("ğŸ¬ Netflix Review Rating Predictor")
    st.write("Type a Netflix app review and get a predicted star rating (1â€“5 â­).")

    with st.expander("ğŸ“Œ Introduction & Background", expanded=False):
        st.write("""
Natural Language Processing (NLP) enables the efficient analysis of large volumes of user reviews, allowing companies to understand customer sentiment beyond simple star ratings. For streaming platforms like Netflix, app store reviews offer valuable insight into user satisfaction, usability, and content quality. NLP makes it possible to systematically identify sentiment trends and recurring themes that would be impractical to analyze manually.
""")

    with st.expander("ğŸ¯ Motivation", expanded=False):
        st.write("""
Analyzing Netflix reviews with NLP helps uncover the specific factors driving user satisfaction or frustration, such as content quality or technical issuesâ€”information not fully captured by ratings alone. Understanding linguistic differences between positive and negative reviews supports data-driven improvements, early issue detection, and better user-experience planning.
""")

    with st.expander("ğŸ“‚ Dataset Description", expanded=False):
        st.write("""
The analysis uses the Kaggle dataset â€œNetflix Reviews with NLP,â€ containing 113,068 mobile app reviews with review text, 1â€“5 star ratings, and metadata. Duplicate review IDs were removed during preprocessing. The dataset is highly imbalanced, with one-star (â‰ˆ39%) and five-star (â‰ˆ28%) reviews dominating, indicating polarized user sentiment and motivating deeper linguistic analysis across rating levels.
""")

    review_text = st.text_area(
        "Enter your review text:",
        placeholder="Example: The app keeps crashing after the update. Very frustrating...",
        height=180
    )

    if st.button("Predict Rating â­", use_container_width=True):
        if not review_text.strip():
            st.warning("Please type a review first.")
        else:
            prediction = model.predict([review_text])[0]
            st.success(f"â­ Predicted Rating: {prediction} / 5")


# =========================
# TAB 2: Findings + PNGs (NO expanders)
# =========================
with tab2:
    st.header("ğŸ“Š Findings & Visual Results (Report)")

    # 1) Ratings Distribution
    st.subheader("1) Ratings Distribution (1â€“5 Stars)")
    st.image("images/ratings_distribution_plots .png", use_container_width=True)
    st.write("""
**Finding:** The dataset is highly imbalanced.  
1â˜… reviews are the largest group and 5â˜… reviews are the second largest.  
Mid-range ratings (2â˜…â€“4â˜…) are much smaller, which makes it harder for models to learn and predict those middle classes accurately.
""")

    st.markdown("---")

    # 2) Text Preprocessing Example
    st.subheader("2) Text Preprocessing (Original vs Cleaned)")
    st.image("images/original_vs_cleaned_side-by-side.png", use_container_width=True)
    st.write("""
**Finding:** Cleaning makes text more consistent for modeling.  
After cleaning (lowercase, removing punctuation/stopwords, lemmatization), the review becomes shorter and focuses on meaningful words.
""")

    st.markdown("---")

    # 3) Sentiment Summary Table (VADER)
    st.subheader("3) Sentiment Analysis Summary (VADER Results)")
    st.image("images/sentiment_analysis.png", use_container_width=True)
    st.write("""
**Finding:** Sentiment matches rating very clearly.  
- Review counts: Positive â‰ˆ 69,270, Negative â‰ˆ 30,018, Neutral â‰ˆ 13,780  
- Average rating by sentiment: Negative â‰ˆ 1.661, Neutral â‰ˆ 2.085, Positive â‰ˆ 3.456  
Also, the average sentiment score increases steadily from 1â˜… to 5â˜… (more positive sentiment â†’ higher rating).
""")

    st.markdown("---")

    # 4) Sentiment vs Rating Count Chart
    st.subheader("4) Sentiment Label Distribution Across Ratings")
    st.image("images/Sentiment_vs_rating_count.png", use_container_width=True)
    st.write("""
**Finding:**  
- 1â˜… reviews contain a lot of **negative** sentiment.  
- 5â˜… reviews are mostly **positive** sentiment.  
- Mid-range ratings (2â˜…â€“4â˜…) show more **mixed/neutral** sentiment, which is one reason these classes are harder to predict.
""")

    st.markdown("---")

    # 5) Correlation Heatmap
    st.subheader("5) Correlation Heatmap: Rating, Sentiment & Text Length")
    st.image("images/correlation_heatmap_rating,_sentiment.png", use_container_width=True)
    st.write("""
**Finding (from the numbers in the heatmap):**  
- Rating vs Sentiment score has a **moderate positive correlation (~0.56)** â†’ more positive sentiment tends to mean higher rating.  
- Word count and character count are **almost perfectly correlated (~0.99)** â†’ they measure the same â€œlengthâ€ signal.  
- Rating vs length is **slightly negative** (about -0.17 to -0.19), meaning longer reviews do not necessarily mean higher ratings.
""")

    st.markdown("---")

    # 6) Top 20 Most Common Words
    st.subheader("6) Top 20 Most Common Words (All Reviews)")
    st.image("images/top_20_most_common_word.png", use_container_width=True)
    st.write("""
**Finding:** Most frequent words are common stopwords (the, i, to, and, itâ€¦).  
This shows why stopword removal is importantâ€”these words appear a lot but do not help predict ratings.
""")

    st.markdown("---")

    # 7) Top Words in 1â˜… vs 5â˜… Reviews
    st.subheader("7) Top Words: 1â˜… Reviews vs 5â˜… Reviews")
    st.image("images/top_words_1_&_5_star_reviews.png", use_container_width=True)
    st.write("""
**Finding:** Word choices differ strongly between negative and positive reviews.  
- 1â˜… reviews contain more frustration/negation words (e.g., **not**, app, netflix, etc.).  
- 5â˜… reviews contain more positive words (e.g., **love**, watch, movies, best, etc.).  
This supports the idea that language clearly reflects user sentiment.
""")

    st.markdown("---")

    # 8) Model Performance Summary (table image)
    st.subheader("8) Final Model Performance Summary (Classical ML + Deep Learning)")
    st.image("images/model_performamce_summary.png", use_container_width=True)
    st.write("""
**Finding (from your table):**  
- Best results are around **~0.645 accuracy** (Bi-LSTM â‰ˆ 0.645, Logistic Regression â‰ˆ 0.645).  
- Linear SVM / Multinomial NB / XGBoost are slightly lower.  
- Balanced Logistic Regression shows **lower overall accuracy** (because it reduces bias toward the big classes).  
- BERT was **not completed** due to environment/framework constraints.
""")

    st.markdown("---")

    # 9) Logistic Regression Classification Report
    st.subheader("9) Logistic Regression (TF-IDF) â€“ Classification Report")
    st.image("images/training_model_logistic_regression.png", use_container_width=True)
    st.write("""
**Finding:** The model is strong on extreme ratings but weak on mid-range.  
- 1â˜… recall â‰ˆ **0.92**, 5â˜… recall â‰ˆ **0.83** (very good)  
- 2â˜… recall â‰ˆ **0.04** (very hard)  
This confirms: text predicts polarized sentiment well, but mixed mid-range classes are difficult.
""")

    st.markdown("---")

    # 10) Confusion Matrix (Logistic Regression)
    st.subheader("10) Confusion Matrix â€” Logistic Regression (TF-IDF)")
    st.image("images/confusion_matrix_logistic_regression.png", use_container_width=True)
    st.write("""
**Finding:** The confusion matrix shows strong accuracy for 1â˜… and 5â˜….  
Mid-range ratings (2â˜…â€“4â˜…) are often misclassified toward 1â˜… or 5â˜…, because their language is more mixed/neutral and the dataset is imbalanced.
""")


    st.markdown("---")

    # 11) Confusion Matrix (Bi-LSTM)
    st.subheader("11) Confusion Matrix â€” Bi-LSTM Model")
    st.image("images/confusion_matrix_bi-LSTM.png", use_container_width=True)
    st.write("""
**Finding:** The Bi-LSTM model shows performance patterns very similar to Logistic Regression.  
- Extreme ratings (1â˜… and 5â˜…) are predicted very well.  
- 1â˜… reviews have very high correct classification counts, and 5â˜… reviews are also strongly identified.  
- Mid-range ratings (2â˜…â€“4â˜…) are frequently misclassified, often shifting toward extreme categories.  

This confirms that even deep learning models struggle with subtle or mixed sentiment, while polarized sentiment is easier to predict from text alone.
""")
